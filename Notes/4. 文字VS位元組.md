# Ch4. 文字 VS. 位元組

包含：數字、字串和布林。

## 字元問題

- 字串就是一系列的字元，最佳的字元定義是`Unicode`，Python3 str取出的就是Unicode字元，Unicode明確地使用特定的位元組來區分字元的身分(code print)。
- 字碼(code print)是從0到1,114,111(十進位)的數字，在`Unicode`中用4~6個十六進位數字，並在前面加上U+，例如：A(U+0041)，在`Unicode6.3`中有10%的有效字碼有被指派字元，Python3.4也使用這個。在`UTF-8`編碼中，A字碼被編為一個位元組\x41，在`UTF-16LE`是\x41\x00兩位元組，強調該值為原始資料而非數值。
- 將字碼轉成位元組是`編碼`，將位元組轉換成字碼是`解碼`。bytes值是以b開頭的，範例：
    ```python
    s = 'café'
    len(s)
    4
    b = s.encode('utf8')
    b'caf\xc3\xa9'
    len(b) #5
    b.decode('utf8')
    café
    ```

### 位元組綱要

- byte and bytearray
1. 每個項目都是0~255的整數
2. bytes的slice也是bytes(就算是單一位元組的slice)
3. bytearray被顯示的時候為bytearray()，以及常值bytes的引數
4. bytearray的slice也是bytearray
範例：

    ```python
    cafe = bytes('café', encoding='utf_8') 
    b'caf\xc3\xa9'
    cafe[0] 
    99
    cafe[:1] 
    b'c'
    cafe_arr = bytearray(cafe) 
    bytearray(b'caf\xc3\xa9')
    cafe_arr[-1:] 
    bytearray(b'\xa9')
    ```
P.S. 從空格到`~`都是屬於可列印的ASCII範圍的位元組，這就是爲什麼b'caf\xc3\xa9'前三個位元組會是b'caf'

5. byte和bytearray都支援各種str語法(除了format、format_map)，replace、upper...re的正規表達式也可以使用，
有一個str沒有的`fromhex`可以解析十六進位

    ```python
    bytes.formhex('31 4B CE A9') 
    b'1K\xce\xa9
    ```

6. 用類緩衝區的物件建立二進位序列，是一種不好的做法，需要做型態轉換，範例：

    ```python
    import array
    numbers = array.array('h', [-2, -1, 0, 1, 2]) #'h'會建立一個短整數(16位元)陣列
    octets = bytes(numbers) #複製位元組到octets
    b'\xfe\xff\xff\xff\x00\x00\x01\x00\x02\x00' #10個byte代表五個短整數
    ```

#### 結構與記憶體資訊

1. struct有許多韓式可以將壓縮的位元組解析成tuple，反之亦然。struct必須使用bytes、bytearray與memoryview物件
範例:使用memoryview與struct來檢視GIF圖像標頭

    ```python
    import struct
    fmt = '<3s3sHH' # < little-endian ; 3s3s 兩個序列的3bytes ; HH 兩個16位元整數
    with open('filter.gif', 'rb') as fp:
        img = memoryview(fp.read()) # 由記憶體內的檔案建立memoryview
    header = img[:10] #建立另一個memoryview;這裡不會複製任何位元組
    bytes(header)
    b'GIF89a+\x02\xe6\x00'
    struct.unpack(fmt, header) #解壓縮成類型、版本、寬、高的tuple
    (b'GIF', b'89a', 555, 230)
    del header
    del img #釋出記憶體
    ```

#### 基本編碼／解碼器

1. python有超過一百種的轉碼器例如:`utf_8`可以在`open()`、`str.encode()`、`bytes.decode()`等函式中帶入使用
範例：

    ```python
    for codec in ['latin_1', 'utf_8', 'utf_16']"
        print(codec, 'El Niño'.encode(codec), sep='\t')
    latin_1 b'El Ni\xf1o'
    utf_8 b'El Ni\xc3\xb1o'
    utf_16 b'\xff\xfeE\x00l \x00N\x00i\x00\xf1\x00o\x00'
    ```

- latin_1 又稱為iso8859_1 很重要，它是其他編碼器的基礎
- utf-8 Web上最常見的8位元編碼類型
- cp1252 Microsoft的latin1超集合，加入彎引號及其他好用的符號

#### 了解編碼／解碼的問題

- python有一種通用的UnicodeError，但通常會比較具體(UnicodeEncodeError、UnicodeDecodeError、SyntaxError)

1. 處理UnicodeEncodeError
大部分的UTF轉碼器只能處理小部分Unicode字元。所以在轉換時如果有字元沒有被定義就會發生此錯誤
範例:

    ```python
    city = 'São Paulo'
    city.encode('utf_8')
    -> b'S\xc3\xa3o Paulo'
    city.encode('cp437')
    -> UnicodeEncodeError
    city.encode('cp437', errors='ignore') #跳過錯誤處理 不推薦使用
    -> b'S?o Paulo
    city.encode('cp437', errors='xmlcharrefreplace') #將無法編碼的字給替換 使用者可以知道發生問題
    -> b'S&#227;o Paulo'
    ```

2. 處理UnicodeDecodeError
並非每個位元組都存有有效的ASCII字元，當轉換這種位元組時就會出現UnicodeDecodeError
範例:

    ```python
    octets = b'Montr\xe9a1'
    octets.decode('cp1252')
    -> 'Montréal'
    octers.decode('utf_8')
    -> UnicodeDecodeError
        octers.decode('utf_8', errors='replace') #無法處理的取代成未知字元
    -> 'Montr?al'
    ```

3. SyntaxError
如果載入一個內含非UTF-8資料的.py檔案且沒有編碼宣告的模組，會看到SyntaxError
UTF-8被廣泛地使用在GNU/Linux與OSX系統，開啟使用cp1252的Windows所建立的.py檔，很有可能遇到這個錯誤，
甚至在Windows的python中也會有此情況，因為在任何平台上python3的預設編碼都是UTF-8

4. 如何找出Byte序列的編碼方式？
最簡單的答案是：無計可施。必須有人告訴你
有些通訊協定與檔案格式例如:HTTP、XML都有標頭清楚告訴我們內容是如何被編碼的。
Byte序列只能確定有些位元組不是ASCII因為他們保存的byte值超過127，但其他的並沒法100%認為他是ASCII或UTF-8

Chardet套件 可辨識30種編碼

    ```python
    chardetect 04-txt-byte-asciidoc
    04-txt-byte-asciidoc: uft-8 with confidence 0.99
    ```

#### 處理文字檔案

- 處理文字檔案最佳做法是`Unicode三明治`，大多數的web框架都採取這種模式(Django)
bytes -> str 輸入時解碼bytes
100%str 只處理str
str -> bytes 輸出時編碼

- python3內建的open會在讀取時進行必要的解碼，並在寫入檔案時進行編碼

    ```python
    open('cafe.txt', 'w', encoding='utf_8'.wirte('café'))
    -> 4
    open('cafe.txt').read()
    -> 'cafÂ©' #寫入檔案時指定utf_8 但在讀檔時沒有指定則出錯了，因為python預設他是系統的預設編碼(windows 1252)
    ```

#### 正規化Unicode來比較

- Unicode有組合字元，所以字串比較相當複雜
例如：café可能會以兩種方式來組合，使用四或五個字馬，但結果看起來一模一樣

    ```python
    s1 = 'café'
    s2 = 'cafe\u0301'
    s1 , s2
    -> ('café', 'café')
    len(s1), len(s2)
    -> (4, 5)
    s1 == s2
    -> False
    ```

- 字碼`U+0301`是combining acute accent在`e`的後面使用就會變成`é`這種被稱作"典型對等物 canonical equivalents"
應用程式會把他視為相同東西，但是python則會視為不相同。
- 解決方法：unicodedata.normalize提供的Unicode正規化，這個函示需要帶入第一個引數有以下四種

- NFC (Normalization From C) and NFD
NFC負責組合字碼產生最簡短的字串，NFD則是負責拆解將組合字元擴展成基本字元，就可以讓比較結果一如預期

    ```python
    s1 = 'café'
    s2 = 'cafe\u0301'
    s1 , s2
    -> ('café', 'café')
    len(normalize('NFC', s1)), len(normalize('NFC', s2))
    -> (4, 4)
    len(normalize('NFD', s1)), len(normalize('NFD', s2))
    -> (5, 5)
    normalize('NFC', s1) == normalize('NFC', s2)
    -> True
    normalize('NFD', s1) == normalize('NFD', s2)
    -> True
    ```

- NFKC and NFKD
K代表“相容性”，在這兩種中每個相容性字元都會被“相容性分解”換成一個或多個字元
舉例：二分之一符號½(U+00BD)會變成三個字元的序列'1/2'
P.S. 相容性字元 micro符號`µ`(U+00B5)和希臘字母(U+03BC)一樣

    ```python
    from unicodedata import normalize, name
    half = '½'
    normalize('NFKC', half)
    -> '1/2'
    ```
NFKC或NFKD可能會損失或曲解資訊，雖然1/2取代½很合理的，但如果是4的2次方會轉換成42會改變他的意思
因此只在特殊情況下使用這兩種而不是永久保存
